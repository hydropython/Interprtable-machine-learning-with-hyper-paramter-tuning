{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7acb1062-078b-42b5-a28e-90d7a8989915",
   "metadata": {},
   "source": [
    "# **Time series weather product pridiction **\n",
    "#### *Learning Objective: Three Tree algorithm application and ANN(MLP) \n",
    "\n",
    "## Step 1: Import neccsary pakageses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45f62982-3291-4c36-8ffc-f60c990d1364",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PCP</th>\n",
       "      <th>Tmin</th>\n",
       "      <th>Tmax</th>\n",
       "      <th>Month</th>\n",
       "      <th>year</th>\n",
       "      <th>Tmean</th>\n",
       "      <th>Rhmax</th>\n",
       "      <th>Rhmin</th>\n",
       "      <th>Rhmean</th>\n",
       "      <th>Evapoavg</th>\n",
       "      <th>WS</th>\n",
       "      <th>SH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1993-01-01</th>\n",
       "      <td>0.0</td>\n",
       "      <td>20.8</td>\n",
       "      <td>31.6</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>26.20</td>\n",
       "      <td>75.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>57.5</td>\n",
       "      <td>3.471109</td>\n",
       "      <td>1.8</td>\n",
       "      <td>8.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-01-02</th>\n",
       "      <td>0.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>24.90</td>\n",
       "      <td>81.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>3.421044</td>\n",
       "      <td>1.6</td>\n",
       "      <td>8.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-01-03</th>\n",
       "      <td>0.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>23.50</td>\n",
       "      <td>89.0</td>\n",
       "      <td>41.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>4.001721</td>\n",
       "      <td>1.2</td>\n",
       "      <td>10.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-01-04</th>\n",
       "      <td>0.0</td>\n",
       "      <td>12.6</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>22.30</td>\n",
       "      <td>91.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>68.5</td>\n",
       "      <td>3.618233</td>\n",
       "      <td>2.1</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1993-01-05</th>\n",
       "      <td>0.0</td>\n",
       "      <td>16.2</td>\n",
       "      <td>33.8</td>\n",
       "      <td>1</td>\n",
       "      <td>1993</td>\n",
       "      <td>25.00</td>\n",
       "      <td>82.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>62.5</td>\n",
       "      <td>3.844625</td>\n",
       "      <td>1.4</td>\n",
       "      <td>9.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-27</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>20.75</td>\n",
       "      <td>83.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>54.5</td>\n",
       "      <td>1.150054</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-28</th>\n",
       "      <td>0.0</td>\n",
       "      <td>8.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>20.20</td>\n",
       "      <td>86.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>56.5</td>\n",
       "      <td>3.723602</td>\n",
       "      <td>0.9</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-29</th>\n",
       "      <td>0.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>20.50</td>\n",
       "      <td>80.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>1.185701</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-30</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10.5</td>\n",
       "      <td>31.0</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>20.75</td>\n",
       "      <td>81.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>52.5</td>\n",
       "      <td>3.683031</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-12-31</th>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>12</td>\n",
       "      <td>2016</td>\n",
       "      <td>18.70</td>\n",
       "      <td>78.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>50.5</td>\n",
       "      <td>3.678237</td>\n",
       "      <td>1.1</td>\n",
       "      <td>10.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8766 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            PCP  Tmin  Tmax  Month  year  Tmean  Rhmax  Rhmin  Rhmean  \\\n",
       "Date                                                                    \n",
       "1993-01-01  0.0  20.8  31.6      1  1993  26.20   75.0   40.0    57.5   \n",
       "1993-01-02  0.0  17.8  32.0      1  1993  24.90   81.0   44.0    62.5   \n",
       "1993-01-03  0.0  14.0  33.0      1  1993  23.50   89.0   41.0    65.0   \n",
       "1993-01-04  0.0  12.6  32.0      1  1993  22.30   91.0   46.0    68.5   \n",
       "1993-01-05  0.0  16.2  33.8      1  1993  25.00   82.0   43.0    62.5   \n",
       "...         ...   ...   ...    ...   ...    ...    ...    ...     ...   \n",
       "2016-12-27  0.0  10.5  31.0     12  2016  20.75   83.0   26.0    54.5   \n",
       "2016-12-28  0.0   8.4  32.0     12  2016  20.20   86.0   27.0    56.5   \n",
       "2016-12-29  0.0   9.0  32.0     12  2016  20.50   80.0   28.0    54.0   \n",
       "2016-12-30  0.0  10.5  31.0     12  2016  20.75   81.0   24.0    52.5   \n",
       "2016-12-31  0.0   6.0  31.4     12  2016  18.70   78.0   23.0    50.5   \n",
       "\n",
       "            Evapoavg   WS    SH  \n",
       "Date                             \n",
       "1993-01-01  3.471109  1.8   8.4  \n",
       "1993-01-02  3.421044  1.6   8.3  \n",
       "1993-01-03  4.001721  1.2  10.5  \n",
       "1993-01-04  3.618233  2.1   9.2  \n",
       "1993-01-05  3.844625  1.4   9.8  \n",
       "...              ...  ...   ...  \n",
       "2016-12-27  1.150054  0.8   0.1  \n",
       "2016-12-28  3.723602  0.9  10.3  \n",
       "2016-12-29  1.185701  1.0   0.2  \n",
       "2016-12-30  3.683031  1.0  10.2  \n",
       "2016-12-31  3.678237  1.1  10.3  \n",
       "\n",
       "[8766 rows x 12 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#Step 1: Read and explore data\n",
    "file='C:/Users/Addisu/Desktop/kidi files/kidi/Evap/Data/Observed/Metehara/DEVAPll.csv'\n",
    "raw_data = pd.read_csv(file, parse_dates = ['Date'],\n",
    "                       index_col = 'Date')\n",
    "dfml = raw_data.copy()\n",
    "dfml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593f9f18-73ec-43bc-abbb-c63f6aa63e43",
   "metadata": {},
   "source": [
    "## Gradient Booster Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a2c7f52a-68ae-429a-908e-86e9aa24eae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'alpha': 0.9,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'friedman_mse',\n",
      " 'init': None,\n",
      " 'learning_rate': 0.1,\n",
      " 'loss': 'squared_error',\n",
      " 'max_depth': 3,\n",
      " 'max_features': None,\n",
      " 'max_leaf_nodes': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_iter_no_change': None,\n",
      " 'random_state': 42,\n",
      " 'subsample': 1.0,\n",
      " 'tol': 0.0001,\n",
      " 'validation_fraction': 0.1,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "{'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "Fitting 7 folds for each of 10 candidates, totalling 70 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "35 fits failed out of a total of 70.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "35 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of GradientBoostingRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.98509901        nan        nan        nan        nan 0.97940541\n",
      " 0.98552362 0.98611884        nan 0.98392854]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.3112 degrees.\n",
      "Accuracy = 90.06%.\n",
      "Score (RMSE): 0.40657626316808543\n",
      "Test MAE  : 0.311\n",
      "Test MSE  : 0.165\n",
      "Model Performance\n",
      "Average Error: 0.0970 degrees.\n",
      "Accuracy = 97.11%.\n",
      "Score (RMSE): 0.16293416760063073\n",
      "Test MAE  : 0.097\n",
      "Test MSE  : 0.027\n",
      "Improvement of 7.83%.\n",
      "LSTM:\n",
      "Nash-Sutcliff-Efficiency-LSTM: 0.9656\n",
      "LSTM:\n",
      "Kling-Gupta-Efficiency-LSTM: 0.9444\n",
      "LSTM:\n",
      "Relative root mean square error-LSTM: 0.0395\n",
      "LSTM:\n",
      "Nash-Sutcliff-Efficiency-LSTM: 0.7859\n",
      "LSTM:\n",
      "Kling-Gupta-Efficiency-LSTM: 0.5810\n",
      "LSTM:\n",
      "Relative root mean square error-LSTM: 0.0983\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-0.9598815487477081"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# GRADIENT BOOSTER\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import matplotlib as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn import metrics, svm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes  import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "\n",
    "# Read data from  csv files\n",
    "X = dfml.drop([\"Evapoavg\"], axis=1)\n",
    "y= dfml[\"Evapoavg\"]\n",
    "Input_data_features = X.values\n",
    "Input_data_labels =y.values\n",
    "\n",
    "\n",
    "# standardize input features X and output labels Y\n",
    "#scaler_standardized_X = StandardScaler()\n",
    "#Input_data_features = scaler_standardized_X.fit_transform(Input_data_features)\n",
    "\n",
    "#scaler_standardized_Y = StandardScaler()\n",
    "#Input_data_labels = scaler_standardized_Y.fit_transform(Input_data_labels)\n",
    "\n",
    "\n",
    "# Split dataset into train, validation, an test\n",
    "index_X_Train_End = int(0.9 * len(Input_data_features))\n",
    "index_X_Validation_End = int(0.9 * len(Input_data_features))\n",
    "\n",
    "X_train = Input_data_features[0: index_X_Train_End]\n",
    "X_valid = Input_data_features[index_X_Train_End: index_X_Validation_End]\n",
    "X_test = Input_data_features[index_X_Validation_End:]\n",
    "Y_valid = Input_data_labels[index_X_Train_End: index_X_Validation_End]\n",
    "Y_test = Input_data_labels[index_X_Validation_End:]\n",
    "Y_train = Input_data_labels[0: index_X_Train_End]\n",
    "\n",
    "\n",
    "# gradiant booster\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor(random_state = 42)\n",
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(gb.get_params())\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf}\n",
    "               #'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "gb = GradientBoostingRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "gb_random = RandomizedSearchCV(estimator = gb, \n",
    "                               param_distributions = random_grid, \n",
    "                               n_iter = 10, \n",
    "                               cv = 7, \n",
    "                               verbose=2, \n",
    "                               random_state=42, \n",
    "                               n_jobs = -1)\n",
    "# Fit the random search model\n",
    "gb_random.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "gb_random.best_params_\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "base_model = GradientBoostingRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, Y_train)\n",
    "base_accuracy = evaluate(base_model,X_test, Y_test)\n",
    "gbb = base_model.predict(X_test)\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "gbb = base_model.predict(X_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(Y_test,gbb))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('Test MAE  : %.3f'%mean_absolute_error(Y_test,gbb))\n",
    "#print('Train MAE : %.3f'%mean_absolute_error(_train, lin_reg.predict(X_train)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "print('Test MSE  : %.3f'%mean_squared_error(Y_test,gbb))\n",
    "\n",
    "\n",
    "\n",
    "best_random = gb_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, Y_test)\n",
    "gbbe = best_random.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "rf = best_random.predict(X_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(Y_test,gbbe))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('Test MAE  : %.3f'%mean_absolute_error(Y_test,gbbe))\n",
    "#print('Train MAE : %.3f'%mean_absolute_error(_train, lin_reg.predict(X_train)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "print('Test MSE  : %.3f'%mean_squared_error(Y_test,gbbe))\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "\n",
    "\n",
    "def relative_root_mean_squared_error(actual, predictions,model_name):\n",
    "    num = np.sum(np.square(actual - predictions))\n",
    "    den = np.sum(np.square(predictions))\n",
    "    squared_error = num/den\n",
    "    rrmse = np.sqrt(squared_error)\n",
    "    print(model_name + ':')\n",
    "    print('Relative root mean square error-LSTM: {:.4f}'.format(rrmse))\n",
    "    return rrmse\n",
    "def nse(actual, predictions,model_name):\n",
    "    nse=(1-(np.sum((actual-predictions)**2)/np.sum((actual-np.mean(actual))**2)))\n",
    "    print(model_name + ':')\n",
    "    print('Nash-Sutcliff-Efficiency-LSTM: {:.4f}'.format(nse))\n",
    "    return nse\n",
    "from scipy import stats\n",
    "def calc_kge(actual, predictions,model_name):\n",
    "    \"\"\"Calculate the Kling-Gupta-Efficiency.\n",
    "    \n",
    "    Calculate the original KGE value following [1].\n",
    "\n",
    "    Args:\n",
    "        obs: Array of the observed values\n",
    "        sim: Array of the simulated values\n",
    "\n",
    "    Returns:\n",
    "        The KGE value for the simulation, compared to the observation.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the arrays are not of equal size or have non-numeric\n",
    "            values.\n",
    "        TypeError: If the arrays is not a supported datatype.\n",
    "        RuntimeError: If the mean or the standard deviation of the observations\n",
    "            equal 0.\n",
    "    \n",
    "    [1] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). \n",
    "    Decomposition of the mean squared error and NSE performance criteria: \n",
    "    Implications for improving hydrological modelling. Journal of Hydrology, \n",
    "    377(1-2), 80-91.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Validation check on the input arrays\n",
    "    #actual = validate_array_input(actual, np.float64, 'actual')\n",
    "    #predictions = validate_array_input(predictions, np.float64, 'predictions')\n",
    "    \n",
    "    if len(actual) != len(predictions):\n",
    "        raise ValueError(\"Arrays must have the same size.\")\n",
    "     \n",
    "    mean_actual = np.mean(actual)\n",
    "    if mean_actual == 0:\n",
    "        msg = \"KGE not definied if the mean of the observations equals 0.\"\n",
    "        raise RuntimeError(msg)\n",
    "    \n",
    "    std_actual = np.std(actual)\n",
    "    if std_actual == 0:\n",
    "        msg = [\"KGE not definied if the standard deviation of the \",\n",
    "               \"observations equals 0.\"]\n",
    "        raise RuntimeError(\"\".join(msg))\n",
    "    actual = np.ndarray.flatten(actual)\n",
    "    predictions = np.ndarray.flatten(predictions)\n",
    "    r = stats.pearsonr(actual, predictions)[0]\n",
    "    alpha = np.std(predictions) / std_actual\n",
    "    beta = np.mean(predictions) / mean_actual\n",
    "    \n",
    "    kge_val = 1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2)\n",
    "    print(model_name + ':')\n",
    "    print('Kling-Gupta-Efficiency-LSTM: {:.4f}'.format(kge_val))\n",
    "    return kge_val \n",
    "def pbias(actual, predictions,model_name):\n",
    "    pbias=100 *(sum((actual - predictions)/sum(actual)))\n",
    "    #print(model_name + ':')\n",
    "    #print('Percent-of-Bias-bilstm: {:.4f}'.format(pbias))\n",
    "    return pbias\n",
    "\n",
    "nse(Y_test, gbbe,'LSTM')\n",
    "calc_kge(Y_test, gbbe,'LSTM')\n",
    "relative_root_mean_squared_error(Y_test, gbbe,'LSTM')\n",
    "#evaluate(Y_test,rfr, 'LSTM')\n",
    "PBIAS_BILSTM=pbias(Y_test, gbbe ,'LSTMLSTM')\n",
    "PBIAS_BILSTM\n",
    "\n",
    "\n",
    "nse(Y_test, gbb,'LSTM')\n",
    "calc_kge(Y_test, gbb,'LSTM')\n",
    "relative_root_mean_squared_error(Y_test, gbb,'LSTM')\n",
    "#evaluate(Y_test,rfr, 'LSTM')\n",
    "PBIAS_BILSTM=pbias(Y_test, gbb ,'LSTMLSTM')\n",
    "PBIAS_BILSTM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00aa40b-ae0d-4184-b610-3d418c4f8b30",
   "metadata": {},
   "source": [
    "## Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b0e6a7-c8ab-46f7-8c06-1483af7b7b89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters currently in use:\n",
      "\n",
      "{'bootstrap': True,\n",
      " 'ccp_alpha': 0.0,\n",
      " 'criterion': 'squared_error',\n",
      " 'max_depth': None,\n",
      " 'max_features': 1.0,\n",
      " 'max_leaf_nodes': None,\n",
      " 'max_samples': None,\n",
      " 'min_impurity_decrease': 0.0,\n",
      " 'min_samples_leaf': 1,\n",
      " 'min_samples_split': 2,\n",
      " 'min_weight_fraction_leaf': 0.0,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'oob_score': False,\n",
      " 'random_state': 42,\n",
      " 'verbose': 0,\n",
      " 'warm_start': False}\n",
      "{'bootstrap': [True, False],\n",
      " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
      " 'max_features': ['auto', 'sqrt'],\n",
      " 'min_samples_leaf': [1, 2, 4],\n",
      " 'min_samples_split': [2, 5, 10],\n",
      " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "15 fits failed out of a total of 30.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "13 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "2 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestRegressor must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.96520049 0.96684424        nan 0.97015576        nan 0.97150552\n",
      "        nan 0.95112996        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.0326 degrees.\n",
      "Accuracy = 99.08%.\n",
      "Score (RMSE): 0.04498803176108161\n",
      "Test MAE  : 0.033\n",
      "Test MSE  : 0.002\n",
      "Improvement of -1.99%.\n",
      "LSTM:\n",
      "Nash-Sutcliff-Efficiency-LSTM: 0.9974\n",
      "LSTM:\n",
      "Kling-Gupta-Efficiency-LSTM: 0.9897\n",
      "LSTM:\n",
      "Relative root mean square error-LSTM: 0.0108\n",
      "Model Performance\n",
      "Average Error: 0.1052 degrees.\n",
      "Accuracy = 96.40%.\n",
      "Score (RMSE): 0.17056310390881613\n",
      "Test MAE  : 0.105\n",
      "Test MSE  : 0.029\n",
      "LSTM:\n",
      "Nash-Sutcliff-Efficiency-LSTM: 0.9623\n",
      "LSTM:\n",
      "Kling-Gupta-Efficiency-LSTM: 0.8912\n",
      "LSTM:\n",
      "Relative root mean square error-LSTM: 0.0413\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3020464871311909"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#%% RANDOM FOREST \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestRegressor(random_state = 42)\n",
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(rf.get_params())\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "pprint(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 10, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "rf_random.best_params_\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "base_model = RandomForestRegressor(n_estimators = 10, random_state = 42)\n",
    "base_model.fit(X_train, Y_train)\n",
    "base_accuracy = evaluate(base_model,X_test, Y_test)\n",
    "rfr = base_model.predict(X_test)\n",
    "\n",
    "#%%\n",
    "from sklearn import metrics\n",
    "\n",
    "rfr = base_model.predict(X_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(Y_test,rfr))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('Test MAE  : %.3f'%mean_absolute_error(Y_test,rfr))\n",
    "#print('Train MAE : %.3f'%mean_absolute_error(_train, lin_reg.predict(X_train)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "print('Test MSE  : %.3f'%mean_squared_error(Y_test,rfr))\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "def relative_root_mean_squared_error(actual, predictions,model_name):\n",
    "    num = np.sum(np.square(actual - predictions))\n",
    "    den = np.sum(np.square(predictions))\n",
    "    squared_error = num/den\n",
    "    rrmse = np.sqrt(squared_error)\n",
    "    print(model_name + ':')\n",
    "    print('Relative root mean square error-LSTM: {:.4f}'.format(rrmse))\n",
    "    return rrmse\n",
    "def nse(actual, predictions,model_name):\n",
    "    nse=(1-(np.sum((actual-predictions)**2)/np.sum((actual-np.mean(actual))**2)))\n",
    "    print(model_name + ':')\n",
    "    print('Nash-Sutcliff-Efficiency-LSTM: {:.4f}'.format(nse))\n",
    "    return nse\n",
    "from scipy import stats\n",
    "def calc_kge(actual, predictions,model_name):\n",
    "    \"\"\"Calculate the Kling-Gupta-Efficiency.\n",
    "    \n",
    "    Calculate the original KGE value following [1].\n",
    "\n",
    "    Args:\n",
    "        obs: Array of the observed values\n",
    "        sim: Array of the simulated values\n",
    "\n",
    "    Returns:\n",
    "        The KGE value for the simulation, compared to the observation.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the arrays are not of equal size or have non-numeric\n",
    "            values.\n",
    "        TypeError: If the arrays is not a supported datatype.\n",
    "        RuntimeError: If the mean or the standard deviation of the observations\n",
    "            equal 0.\n",
    "    \n",
    "    [1] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). \n",
    "    Decomposition of the mean squared error and NSE performance criteria: \n",
    "    Implications for improving hydrological modelling. Journal of Hydrology, \n",
    "    377(1-2), 80-91.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Validation check on the input arrays\n",
    "    #actual = validate_array_input(actual, np.float64, 'actual')\n",
    "    #predictions = validate_array_input(predictions, np.float64, 'predictions')\n",
    "    \n",
    "    if len(actual) != len(predictions):\n",
    "        raise ValueError(\"Arrays must have the same size.\")\n",
    "     \n",
    "    mean_actual = np.mean(actual)\n",
    "    if mean_actual == 0:\n",
    "        msg = \"KGE not definied if the mean of the observations equals 0.\"\n",
    "        raise RuntimeError(msg)\n",
    "    \n",
    "    std_actual = np.std(actual)\n",
    "    if std_actual == 0:\n",
    "        msg = [\"KGE not definied if the standard deviation of the \",\n",
    "               \"observations equals 0.\"]\n",
    "        raise RuntimeError(\"\".join(msg))\n",
    "    actual = np.ndarray.flatten(actual)\n",
    "    predictions = np.ndarray.flatten(predictions)\n",
    "    r = stats.pearsonr(actual, predictions)[0]\n",
    "    alpha = np.std(predictions) / std_actual\n",
    "    beta = np.mean(predictions) / mean_actual\n",
    "    \n",
    "    kge_val = 1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2)\n",
    "    print(model_name + ':')\n",
    "    print('Kling-Gupta-Efficiency-LSTM: {:.4f}'.format(kge_val))\n",
    "    return kge_val \n",
    "def pbias(actual, predictions,model_name):\n",
    "    pbias=100 *(sum((actual - predictions)/sum(actual)))\n",
    "    #print(model_name + ':')\n",
    "    #print('Percent-of-Bias-bilstm: {:.4f}'.format(pbias))\n",
    "    return pbias\n",
    "\n",
    "nse(Y_test, rfr,'LSTM')\n",
    "calc_kge(Y_test, rfr,'LSTM')\n",
    "relative_root_mean_squared_error(Y_test, rfr,'LSTM')\n",
    "#evaluate(Y_test,rfr, 'LSTM')\n",
    "PBIAS_BILSTM=pbias(Y_test, rfr,'LSTMLSTM')\n",
    "PBIAS_BILSTM\n",
    "\n",
    "best_random = rf_random.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, Y_test)\n",
    "rf = best_random.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "rf = best_random.predict(X_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(Y_test,rf))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "print('Test MAE  : %.3f'%mean_absolute_error(Y_test,rf))\n",
    "#print('Train MAE : %.3f'%mean_absolute_error(_train, lin_reg.predict(X_train)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "print('Test MSE  : %.3f'%mean_squared_error(Y_test,rf))\n",
    "\n",
    "nse(Y_test, rf,'LSTM')\n",
    "calc_kge(Y_test, rf,'LSTM')\n",
    "relative_root_mean_squared_error(Y_test, rf,'LSTM')\n",
    "#evaluate(Y_test,rfr, 'LSTM')\n",
    "PBIAS_BILSTM=pbias(Y_test, rf,'LSTMLSTM')\n",
    "PBIAS_BILSTM\n",
    "\n",
    "#predictions=regressor.predict(send_to_model)\n",
    "\n",
    "#print(predictions)\n",
    "#output=pd.DataFrame({\"EVP\":rf})\n",
    "#output.to_csv('C:/Users/Addisu/Desktop/kidi/Evap/Data/Data For Machine learning/Dubti/New Dubti/RF_IS2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568a221-8778-4faa-8aaa-aeebf852b234",
   "metadata": {},
   "source": [
    "## XGBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2657db29-bb8b-48b4-a131-dca1c543b766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "Parameters currently in use:\n",
      "\n",
      "{'base_score': None,\n",
      " 'booster': None,\n",
      " 'callbacks': None,\n",
      " 'colsample_bylevel': None,\n",
      " 'colsample_bynode': None,\n",
      " 'colsample_bytree': None,\n",
      " 'early_stopping_rounds': None,\n",
      " 'enable_categorical': False,\n",
      " 'eval_metric': None,\n",
      " 'feature_types': None,\n",
      " 'gamma': None,\n",
      " 'gpu_id': None,\n",
      " 'grow_policy': None,\n",
      " 'importance_type': None,\n",
      " 'interaction_constraints': None,\n",
      " 'learning_rate': None,\n",
      " 'max_bin': None,\n",
      " 'max_cat_threshold': None,\n",
      " 'max_cat_to_onehot': None,\n",
      " 'max_delta_step': None,\n",
      " 'max_depth': None,\n",
      " 'max_leaves': None,\n",
      " 'min_child_weight': None,\n",
      " 'missing': nan,\n",
      " 'monotone_constraints': None,\n",
      " 'n_estimators': 100,\n",
      " 'n_jobs': None,\n",
      " 'num_parallel_tree': None,\n",
      " 'objective': 'reg:squarederror',\n",
      " 'predictor': None,\n",
      " 'random_state': 30,\n",
      " 'reg_alpha': None,\n",
      " 'reg_lambda': None,\n",
      " 'sampling_method': None,\n",
      " 'scale_pos_weight': None,\n",
      " 'subsample': None,\n",
      " 'tree_method': None,\n",
      " 'validate_parameters': None,\n",
      " 'verbosity': None}\n",
      "Model Performance\n",
      "Average Error: 0.0231 degrees.\n",
      "Accuracy = 99.35%.\n",
      "Score (RMSE): 0.035247806311638985\n",
      "Test MAE  : 0.023\n",
      "Test MSE  : 0.001\n",
      "Model Performance\n",
      "Average Error: 0.0633 degrees.\n",
      "Accuracy = 97.88%.\n",
      "Score (RMSE): 0.09649538820595005\n",
      "Test MAE  : 0.063\n",
      "Test MSE  : 0.009\n",
      "Improvement of -1.48%.\n",
      "LSTM:\n",
      "Nash-Sutcliff-Efficiency-LSTM: 0.9879\n",
      "LSTM:\n",
      "Kling-Gupta-Efficiency-LSTM: 0.9358\n",
      "LSTM:\n",
      "Relative root mean square error-LSTM: 0.0232\n",
      "LSTM:\n",
      "Nash-Sutcliff-Efficiency-LSTM: 0.9984\n",
      "LSTM:\n",
      "Kling-Gupta-Efficiency-LSTM: 0.9956\n",
      "LSTM:\n",
      "Relative root mean square error-LSTM: 0.0085\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.07894068593436875"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGBOOST\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "parameters = {'objective':['reg:squarederror'],\n",
    "              'booster':['gbtree','gblinear'],\n",
    "              'learning_rate': [0.1], \n",
    "              'max_depth': [7,10,15,20],\n",
    "              'min_child_weight': [10,15,20,25],\n",
    "              'colsample_bytree': [0.8, 0.9, 1],\n",
    "              'n_estimators': [300,400,500,600],\n",
    "              \"reg_alpha\"   : [0.5,0.2,1],\n",
    "              \"reg_lambda\"  : [2,3,5],\n",
    "              \"gamma\"       : [1,2,3]}\n",
    "\n",
    "xgb_model = XGBRegressor(random_state=30)\n",
    "\n",
    "grid_obj_xgb = RandomizedSearchCV(xgb_model,parameters, cv=5,n_iter=5,scoring='neg_mean_absolute_error',verbose=5,n_jobs=12)\n",
    "grid_obj_xgb.fit(X_train, Y_train,verbose = 1)\n",
    "\n",
    "#y_pred_train = grid_obj_xgb.predict(df_train)\n",
    "y_pred_test = grid_obj_xgb.predict(X_test)\n",
    "\n",
    "from pprint import pprint\n",
    "# Look at parameters used by our current forest\n",
    "print('Parameters currently in use:\\n')\n",
    "pprint(xgb_model.get_params())\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "base_model = XGBRegressor(random_state=30)\n",
    "base_model.fit(X_train, Y_train)\n",
    "base_accuracy = evaluate(base_model,X_test, Y_test)\n",
    "xgbb = base_model.predict(X_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(Y_test,xgbb))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('Test MAE  : %.3f'%mean_absolute_error(Y_test,xgbb))\n",
    "#print('Train MAE : %.3f'%mean_absolute_error(_train, lin_reg.predict(X_train)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "print('Test MSE  : %.3f'%mean_squared_error(Y_test,xgbb))\n",
    "\n",
    "\n",
    "\n",
    "y_pred_test = grid_obj_xgb.predict(X_test)\n",
    "grid_obj_xgb.best_params_\n",
    "grid_obj_xgb.best_score_\n",
    "\n",
    "#best_random = kn_random.best_estimator_\n",
    "random_accuracy = evaluate(grid_obj_xgb, X_test, Y_test)\n",
    "y_pred_test = grid_obj_xgb.predict(X_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(Y_test,y_pred_test))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('Test MAE  : %.3f'%mean_absolute_error(Y_test,y_pred_test))\n",
    "#print('Train MAE : %.3f'%mean_absolute_error(_train, lin_reg.predict(X_train)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "print('Test MSE  : %.3f'%mean_squared_error(Y_test,y_pred_test))\n",
    "\n",
    "\n",
    "print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "nse(Y_test,y_pred_test,'LSTM')\n",
    "calc_kge(Y_test,y_pred_test,'LSTM')\n",
    "relative_root_mean_squared_error(Y_test,y_pred_test,'LSTM')\n",
    "#evaluate(Y_test,rfr, 'LSTM')\n",
    "PBIAS_BILSTM=pbias(Y_test,y_pred_test,'LSTMLSTM')\n",
    "PBIAS_BILSTM\n",
    "nse(Y_test,xgbb,'LSTM')\n",
    "calc_kge(Y_test,xgbb,'LSTM')\n",
    "relative_root_mean_squared_error(Y_test,xgbb,'LSTM')\n",
    "#evaluate(Y_test,rfr, 'LSTM')\n",
    "PBIAS_BILSTM=pbias(Y_test,xgbb,'LSTMLSTM')\n",
    "PBIAS_BILSTM\n",
    "#output=pd.DataFrame({\"EVP\":y_pred_test})\n",
    "#output.to_csv('C:/Users/Addisu/Desktop/kidi/Evap/Data/Data For Machine learning/Dubti/New Dubti/XGBoost_IS2.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91faacb6-6edd-4795-bc4a-01927baac687",
   "metadata": {},
   "source": [
    "## MLP Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e2cf74f-6ca3-461a-99c6-ab3ba47f70cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "3 fits failed out of a total of 50.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "3 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 753, in fit\n",
      "    return self._fit(X, y, incremental=False)\n",
      "  File \"C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py\", line 496, in _fit\n",
      "    raise ValueError(\n",
      "ValueError: Solver produced non-finite parameter weights. The input data may contain large values and need to be preprocessed.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\anaconda3\\envs\\jupyterenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [            nan  3.34368398e-01 -3.17685744e-03 -3.32772850e-03\n",
      "             nan -5.02983950e-03  5.82483929e-01 -4.17946602e+10\n",
      "  5.18029772e-01  3.14276334e-01]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Performance\n",
      "Average Error: 0.2063 degrees.\n",
      "Accuracy = 94.26%.\n",
      "Score (RMSE): 0.2640159060574793\n",
      "Test MAE  : 0.206\n",
      "Test MSE  : 0.070\n",
      "Score (RMSE): 0.2640159060574793\n",
      "Test MAE  : 0.206\n",
      "Test MSE  : 0.070\n",
      "Model Performance\n",
      "Average Error: 0.2310 degrees.\n",
      "Accuracy = 93.44%.\n",
      "Score (RMSE): 0.28525420169554694\n",
      "Test MAE  : 0.231\n",
      "Test MSE  : 0.081\n",
      "LSTM:\n",
      "Nash-Sutcliff-Efficiency-LSTM: 0.9097\n",
      "LSTM:\n",
      "Kling-Gupta-Efficiency-LSTM: 0.9193\n",
      "LSTM:\n",
      "Relative root mean square error-LSTM: 0.0623\n",
      "LSTM:\n",
      "Nash-Sutcliff-Efficiency-LSTM: 0.8946\n",
      "LSTM:\n",
      "Kling-Gupta-Efficiency-LSTM: 0.8644\n",
      "LSTM:\n",
      "Relative root mean square error-LSTM: 0.0707\n"
     ]
    }
   ],
   "source": [
    "# MLP\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "mlpr = MLPRegressor(max_iter=7000)\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(10,30,10),(20,)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "}\n",
    "\n",
    "gridCV = RandomizedSearchCV( mlpr,parameter_space,n_jobs=-1, cv=5 )\n",
    "gridCV.fit(X_train, Y_train)\n",
    "mlpr.fit(X_train, Y_train)\n",
    "random_accuracy = evaluate(mlpr, X_test, Y_test)\n",
    "mlpp= mlpr.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "rf = best_random.predict(X_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(Y_test,mlpp))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('Test MAE  : %.3f'%mean_absolute_error(Y_test,mlpp))\n",
    "#print('Train MAE : %.3f'%mean_absolute_error(_train, lin_reg.predict(X_train)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "print('Test MSE  : %.3f'%mean_squared_error(Y_test,mlpp))\n",
    "\n",
    "#print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "score = np.sqrt(metrics.mean_squared_error(Y_test,mlpp))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('Test MAE  : %.3f'%mean_absolute_error(Y_test,mlpp))\n",
    "#print('Train MAE : %.3f'%mean_absolute_error(_train, lin_reg.predict(X_train)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "print('Test MSE  : %.3f'%mean_squared_error(Y_test,mlpp))\n",
    "\n",
    "best_random = gridCV.best_estimator_\n",
    "random_accuracy = evaluate(best_random, X_test, Y_test)\n",
    "mlp= best_random.predict(X_test)\n",
    "from sklearn import metrics\n",
    "\n",
    "rf = best_random.predict(X_test)\n",
    "score = np.sqrt(metrics.mean_squared_error(Y_test,mlp))\n",
    "print(\"Score (RMSE): {}\".format(score))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "print('Test MAE  : %.3f'%mean_absolute_error(Y_test,mlp))\n",
    "#print('Train MAE : %.3f'%mean_absolute_error(_train, lin_reg.predict(X_train)))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_squared_log_error\n",
    "print('Test MSE  : %.3f'%mean_squared_error(Y_test,mlp))\n",
    "\n",
    "#print('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - base_accuracy) / base_accuracy))\n",
    "\n",
    "def relative_root_mean_squared_error(actual, predictions,model_name):\n",
    "    num = np.sum(np.square(actual - predictions))\n",
    "    den = np.sum(np.square(predictions))\n",
    "    squared_error = num/den\n",
    "    rrmse = np.sqrt(squared_error)\n",
    "    print(model_name + ':')\n",
    "    print('Relative root mean square error-LSTM: {:.4f}'.format(rrmse))\n",
    "    return rrmse\n",
    "def nse(actual, predictions,model_name):\n",
    "    nse=(1-(np.sum((actual-predictions)**2)/np.sum((actual-np.mean(actual))**2)))\n",
    "    print(model_name + ':')\n",
    "    print('Nash-Sutcliff-Efficiency-LSTM: {:.4f}'.format(nse))\n",
    "    return nse\n",
    "from scipy import stats\n",
    "def calc_kge(actual, predictions,model_name):\n",
    "    \"\"\"Calculate the Kling-Gupta-Efficiency.\n",
    "    \n",
    "    Calculate the original KGE value following [1].\n",
    "\n",
    "    Args:\n",
    "        obs: Array of the observed values\n",
    "        sim: Array of the simulated values\n",
    "\n",
    "    Returns:\n",
    "        The KGE value for the simulation, compared to the observation.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the arrays are not of equal size or have non-numeric\n",
    "            values.\n",
    "        TypeError: If the arrays is not a supported datatype.\n",
    "        RuntimeError: If the mean or the standard deviation of the observations\n",
    "            equal 0.\n",
    "    \n",
    "    [1] Gupta, H. V., Kling, H., Yilmaz, K. K., & Martinez, G. F. (2009). \n",
    "    Decomposition of the mean squared error and NSE performance criteria: \n",
    "    Implications for improving hydrological modelling. Journal of Hydrology, \n",
    "    377(1-2), 80-91.\n",
    "    \n",
    "    \"\"\"\n",
    "    # Validation check on the input arrays\n",
    "    #actual = validate_array_input(actual, np.float64, 'actual')\n",
    "    #predictions = validate_array_input(predictions, np.float64, 'predictions')\n",
    "    \n",
    "    if len(actual) != len(predictions):\n",
    "        raise ValueError(\"Arrays must have the same size.\")\n",
    "     \n",
    "    mean_actual = np.mean(actual)\n",
    "    if mean_actual == 0:\n",
    "        msg = \"KGE not definied if the mean of the observations equals 0.\"\n",
    "        raise RuntimeError(msg)\n",
    "    \n",
    "    std_actual = np.std(actual)\n",
    "    if std_actual == 0:\n",
    "        msg = [\"KGE not definied if the standard deviation of the \",\n",
    "               \"observations equals 0.\"]\n",
    "        raise RuntimeError(\"\".join(msg))\n",
    "    actual = np.ndarray.flatten(actual)\n",
    "    predictions = np.ndarray.flatten(predictions)\n",
    "    r = stats.pearsonr(actual, predictions)[0]\n",
    "    alpha = np.std(predictions) / std_actual\n",
    "    beta = np.mean(predictions) / mean_actual\n",
    "    \n",
    "    kge_val = 1 - np.sqrt((r-1)**2 + (alpha-1)**2 + (beta-1)**2)\n",
    "    print(model_name + ':')\n",
    "    print('Kling-Gupta-Efficiency-LSTM: {:.4f}'.format(kge_val))\n",
    "    return kge_val \n",
    "def pbias(actual, predictions,model_name):\n",
    "    pbias=100 *(sum((actual - predictions)/sum(actual)))\n",
    "    #print(model_name + ':')\n",
    "    #print('Percent-of-Bias-bilstm: {:.4f}'.format(pbias))\n",
    "    return pbias\n",
    "\n",
    "nse(Y_test, mlpp ,'LSTM')\n",
    "calc_kge(Y_test,mlpp ,'LSTM')\n",
    "relative_root_mean_squared_error(Y_test,mlpp ,'LSTM')\n",
    "#evaluate(Y_test,rfr, 'LSTM')\n",
    "PBIAS_BILSTM=pbias(Y_test, mlpp  ,'LSTMLSTM')\n",
    "\n",
    "def evaluate(model, test_features, test_labels):\n",
    "    predictions = model.predict(test_features)\n",
    "    errors = abs(predictions - test_labels)\n",
    "    mape = 100 * np.mean(errors / test_labels)\n",
    "    accuracy = 100 - mape\n",
    "    print('Model Performance')\n",
    "    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n",
    "    print('Accuracy = {:0.2f}%.'.format(accuracy))\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "PBIAS_BILSTM=pbias(Y_test,mlpp,'LSTMLSTM')\n",
    "PBIAS_BILSTM\n",
    "\n",
    "nse(Y_test, mlp ,'LSTM')\n",
    "calc_kge(Y_test,mlp ,'LSTM')\n",
    "relative_root_mean_squared_error(Y_test,mlp ,'LSTM')\n",
    "#evaluate(Y_test,rfr, 'LSTM')\n",
    "PBIAS_BILSTM=pbias(Y_test, mlp  ,'LSTMLSTM')\n",
    "#output=pd.DataFrame({\"EVP\":mlp})\n",
    "#output.to_csv('C:/Users/Addisu/Desktop/kidi/Evap/Data/Data For Machine learning/Dubti/New Dubti/MLP_IS2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba12b00c-fbe1-40e3-b47c-45a509968c15",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
